{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert path files appropriately for data which can be found after generation in results folder for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = \"res\" # res -> ResNet, dense -> DenseNet, mobile -> MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"1\" # Hyperparameter C value from data\n",
    "locald = './results/pqat/DIVA/'+net+'net/' # Please change to the corresponding results folder for evaluation\n",
    "folderName= net + 'net_imagenet_images_second'\n",
    "filterName= net +'net_imagenet_filters_second'\n",
    "dataFolder= net +'net_imagenet_data_second'\n",
    "dataName= 'second'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats():\n",
    "    advist_data = np.array([])\n",
    "\n",
    "    with open(locald + dataFolder+\"/\"+dataName+'_advdist_data.csv', 'r') as filehandle:\n",
    "        for line in filehandle:\n",
    "            temp_data = line[:-2]\n",
    "\n",
    "        advist_data = np.concatenate((advist_data, np.array(temp_data.split(\", \"))))\n",
    "        \n",
    "    steps_data = np.array([])\n",
    "\n",
    "    with open(locald +dataFolder+\"/\"+dataName+'_steps_data.csv', 'r') as filehandle:\n",
    "        for line in filehandle:\n",
    "            temp_data = line[:-2]\n",
    "\n",
    "        steps_data = np.concatenate((steps_data, np.array(temp_data.split(\", \"))))\n",
    "    \n",
    "    time_data = np.array([])\n",
    "\n",
    "    with open(locald +dataFolder+\"/\"+dataName+'_time_data.csv', 'r') as filehandle:\n",
    "        for line in filehandle:\n",
    "            temp_data = line[:-2]\n",
    "\n",
    "        time_data = np.concatenate((time_data, np.array(temp_data.split(\", \"))))\n",
    "    \n",
    "    advist_datak = np.array([])\n",
    "\n",
    "    with open(locald +dataFolder+\"/\"+dataName+'_advdistk_data.csv', 'r') as filehandle:\n",
    "        for line in filehandle:\n",
    "            temp_data = line[:-2]\n",
    "\n",
    "        advist_datak = np.concatenate((advist_datak, np.array(temp_data.split(\", \"))))\n",
    "        \n",
    "    steps_datak = np.array([])\n",
    "\n",
    "    with open(locald +dataFolder+\"/\"+dataName+'_stepsk_data.csv', 'r') as filehandle:\n",
    "        for line in filehandle:\n",
    "            temp_data = line[:-2]\n",
    "\n",
    "        steps_datak = np.concatenate((steps_datak, np.array(temp_data.split(\", \"))))\n",
    "    \n",
    "    time_datak = np.array([])\n",
    "\n",
    "    with open(locald +dataFolder+\"/\"+dataName+'_timek_data.csv', 'r') as filehandle:\n",
    "        for line in filehandle:\n",
    "            temp_data = line[:-2]\n",
    "\n",
    "        time_datak = np.concatenate((time_datak, np.array(temp_data.split(\", \"))))\n",
    "    \n",
    "    time_data = time_data.astype('float')\n",
    "    advist_data = advist_data.astype('float')\n",
    "    steps_data = steps_data.astype('float')\n",
    "\n",
    "    time_datak = time_datak.astype('float')\n",
    "    advist_datak = advist_datak.astype('float')\n",
    "    steps_datak = steps_datak.astype('float')\n",
    "\n",
    "    time_data_ = np.mean(time_data), np.std(time_data)\n",
    "    advdist_data_ = np.mean(advist_data), np.std(advist_data)\n",
    "    steps_data_ = np.mean(steps_data), np.std(steps_data)\n",
    "\n",
    "    time_datak_ = np.mean(time_datak), np.std(time_datak)\n",
    "    advdist_datak_ = np.mean(advist_datak), np.std(advist_datak)\n",
    "    steps_datak_ = np.mean(steps_datak), np.std(steps_datak)\n",
    "    \n",
    "    print(\"Number of Successes\",len(steps_data))\n",
    "    print(\"Total Time\",np.sum(time_data)) \n",
    "    print(\"Total Steps\",np.sum(steps_data))\n",
    "    print(\"Steps\",steps_data_)\n",
    "    print(\"Time\", time_data_)\n",
    "    \n",
    "    print()\n",
    "    print(\"Time5\",time_datak_) \n",
    "    print(\"Dist5\",advdist_datak_) \n",
    "    print(\"Steps5\",steps_datak_)\n",
    "    \n",
    "    print()\n",
    "    print(\"Number of top1 success\",(len(steps_data))/3000)\n",
    "    print(\"Number of top5 success\",len(steps_datak)/3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"c\", c)\n",
    "print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Custom layers for DenseNet to support quantisation\n",
    "'''\n",
    "\n",
    "class DefaultBNQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "        return []\n",
    "\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "        return []\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "        pass\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "        pass\n",
    "    def get_output_quantizers(self, layer):\n",
    "        return [tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=8, per_axis=False, symmetric=False, narrow_range=False)]\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n",
    "    \n",
    "    \n",
    "class NoOpQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    \"\"\"Use this config object if the layer has nothing to be quantized for \n",
    "    quantization aware training.\"\"\"\n",
    "\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "        return []\n",
    "\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "        return []\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "        pass\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "        pass\n",
    "\n",
    "    def get_output_quantizers(self, layer):\n",
    "        # Does not quantize output, since we return an empty list.\n",
    "        return []\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n",
    "    \n",
    "    \n",
    "def apply_quantization(layer):\n",
    "    if 'bn'  in layer.name:\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer,DefaultBNQuantizeConfig())\n",
    "    elif 'concat' in layer.name:\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer,NoOpQuantizeConfig())\n",
    "    else:\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 224 ,224\n",
    "input_shape = (img_rows, img_cols, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choice of model to evaluate\n",
    "model_type = \"m\" # r -> ResNet, d -> DenseNet, m -> MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == \"r\":\n",
    "    model_ = ResNet50(input_shape=input_shape)\n",
    "    pqat_model = tfmot.quantization.keras.quantize_model(model_)\n",
    "    p_model = ResNet50(input_tensor = pqat_model.input)\n",
    "    model = ResNet50(input_tensor = pqat_model.input)\n",
    "    model.load_weights(\"../weights/fp_model_40_resnet50.h5\")\n",
    "    p_model.load_weights(\"../weights/p_model_40_resnet50.h5\")\n",
    "    pqat_model.load_weights(\"../weights/pqat_model_40_resnet50.h5\")\n",
    "    print(\"ResNet Done\")\n",
    "    \n",
    "elif model_type == \"m\":\n",
    "    model_ = MobileNet(input_shape=input_shape)\n",
    "    pqat_model = tfmot.quantization.keras.quantize_model(model_)\n",
    "    p_model = MobileNet(input_tensor = pqat_model.input)\n",
    "    model = ResNet50(input_tensor = pqat_model.input)\n",
    "    model.load_weights(\"../weights/fp_model_40_mobilenet.h5\")\n",
    "    p_model.load_weights(\"../weights/p_model_40_mobilenet.h5\")\n",
    "    pqat_model.load_weights(\"../weights/pqat_model_40_mobilenet.h5\")\n",
    "    print(\"MobNet Done\")\n",
    "    \n",
    "else:\n",
    "    model_ = tf.keras.applications.DenseNet121(input_shape=(img_rows, img_cols,3))\n",
    "    # Create a base model\n",
    "    base_model = model_\n",
    "    # Helper function uses `quantize_annotate_layer` to annotate that only the \n",
    "    # Dense layers should be quantized.\n",
    "\n",
    "    LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\n",
    "    MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\n",
    "\n",
    "    # Use `tf.keras.models.clone_model` to apply `apply_quantization_to_dense` \n",
    "    # to the layers of the model.\n",
    "    annotated_model = tf.keras.models.clone_model(\n",
    "        base_model,\n",
    "        clone_function=apply_quantization,\n",
    "    )\n",
    "\n",
    "    with tfmot.quantization.keras.quantize_scope({'DefaultBNQuantizeConfig': DefaultBNQuantizeConfig, 'NoOpQuantizeConfig': NoOpQuantizeConfig}):\n",
    "        pqat_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "\n",
    "    model = DenseNet121(input_tensor = pqat_model.input)\n",
    "    p_model = DenseNet121(input_tensor = pqat_model.input)\n",
    "    model.load_weights(\"../weights/fp_model_40_densenet121.h5\")\n",
    "    pqat_model.load_weights(\"../weights/pqat_model_40_densenet121.h5\")\n",
    "    p_model.load_weights(\"../weights/p_model_40_densenet121.h5\")\n",
    "\n",
    "    preprocess = tf.keras.applications.densenet.preprocess_input\n",
    "    decode = tf.keras.applications.densenet.decode_predictions\n",
    "    net = 'dense'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = False\n",
    "p_model.trainable = False\n",
    "pqat_model.trainable = False\n",
    "model.compile()\n",
    "p_model.compile()\n",
    "pqat_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appropriate Preprocessing and Decoding predictions\n",
    "if model_type==\"r\":\n",
    "    preproc = tf.keras.applications.resnet.preprocess_input\n",
    "elif model_type==\"m\":\n",
    "    preproc = tf.keras.applications.mobilenet.preprocess_input\n",
    "else:\n",
    "    preproc = tf.keras.applications.densenet.preprocess_input\n",
    "\n",
    "# Choose model\n",
    "if model_type==\"r\":\n",
    "    dec = tf.keras.applications.resnet.decode_predictions\n",
    "elif model_type==\"m\":\n",
    "    dec = tf.keras.applications.mobilenet.decode_predictions\n",
    "else:\n",
    "    dec = tf.keras.applications.densenet.decode_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Generated Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = {'file_name': tf.TensorSpec(shape=(), dtype=tf.string, name=None),\n",
    " 'image': tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32, name=None),\n",
    " 'label': tf.TensorSpec(shape=(), dtype=tf.int64, name=None)}\n",
    "\n",
    "mydataset = tf.data.experimental.load(\"../datasets/Imagenet/pruning/3kImages/\",es).batch(20).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which model(pruned/pruned+quantized) to evaluate\n",
    "model_p_or_pqat = 'p' # p -> pruned model; qat -> pruned and quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_p_or_pqat == 'p':\n",
    "    q_model = p_model\n",
    "else:\n",
    "    q_model = pqat_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DSSIM Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this job you must have dssim downloaded which can be done byusing their github link, https://github.com/kornelski/dssim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job1(image,fil,og):\n",
    "    \n",
    "    tf.keras.preprocessing.image.save_img(\"./tmp/fake1.png\", image)\n",
    "    tf.keras.preprocessing.image.save_img(\"./tmp/real1.png\", og)\n",
    "    \n",
    "    process = subprocess.run(['./path/to/dssim','./tmp/fake1.png','./tmp/real1.png'], \n",
    "                         stdout=subprocess.PIPE,\n",
    "                         stderr=subprocess.PIPE,\n",
    "                         universal_newlines=True)\n",
    "    if process.stdout.split('\\t')[0] == '':\n",
    "        print(process.stderr)\n",
    "    else:\n",
    "        dssim_data.append(process.stdout.split('\\t')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Delta Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job5(image, fil, og):\n",
    "    \n",
    "    image = np.copy(image)\n",
    "    og = np.copy(og)\n",
    "    \n",
    "    real = np.expand_dims(og, axis=0)\n",
    "    attack = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    pred1, pred2= model.predict(real), q_model.predict(real)\n",
    "    \n",
    "    real_r_pred = dec(pred1, top=1000)\n",
    "    real_q_pred = dec(pred2, top=1000)\n",
    "    \n",
    "    pred1, pred2= model.predict(attack), q_model.predict(attack)\n",
    "    \n",
    "    attack_r_pred = dec(pred1, top=1000)\n",
    "    attack_q_pred = dec(pred2, top=1000)\n",
    "    \n",
    "    r_label = real_r_pred[0][0][1]\n",
    "    \n",
    "    print(r_label)\n",
    "    \n",
    "    rr = float(real_r_pred[0][0][2]) #Correct Label OG Model on OG image\n",
    "    rq = float(real_q_pred[0][0][2]) #Correct Label PRuned Model on OG image\n",
    "    \n",
    "    for i in range(0,1000):\n",
    "        if attack_q_pred[0][i][1] == r_label:\n",
    "            aq = float(attack_q_pred[0][i][2])\n",
    "            break\n",
    "                       \n",
    "    for i in range(0,1000):\n",
    "        if attack_r_pred[0][i][1] == r_label:\n",
    "            ar = float(attack_r_pred[0][i][2])\n",
    "            break\n",
    "    \n",
    "    \n",
    "    confRR_data.append(rr)\n",
    "    confAR_data.append(ar)\n",
    "    confRQ_data.append(rq)\n",
    "    confAQ_data.append(aq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evasion cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job4(image,fil,og):\n",
    "    \n",
    "    image = np.copy(image)\n",
    "    og = np.copy(og)\n",
    "      \n",
    "    real = np.expand_dims(preproc(og), axis=0)\n",
    "    attack = np.expand_dims(preproc(image), axis=0)\n",
    "    \n",
    "    pred1, pred2= model.predict(real), q_model.predict(real)\n",
    "    \n",
    "    r_label = np.argmax(pred1)\n",
    "    \n",
    "    pred1, pred2= model.predict(attack), q_model.predict(attack)\n",
    "    \n",
    "    attack_r_pred = np.argmax(pred1)\n",
    "    attack_q_pred = np.argmax(pred2)\n",
    "    \n",
    "    if (r_label == attack_r_pred and r_label == attack_q_pred):\n",
    "        CC[0] += 1\n",
    "        return\n",
    "    if (r_label != attack_r_pred and r_label != attack_q_pred):\n",
    "        WW[0] += 1\n",
    "        return\n",
    "    if (r_label != attack_r_pred and r_label == attack_q_pred):\n",
    "        WC[0] += 1\n",
    "        return\n",
    "        \n",
    "    print(\"Data might be damaged: \", r_label,\" \",attack_r_pred, \" \",attack_q_pred)\n",
    "    p[0]+=1\n",
    "    if p[0]==2:\n",
    "        assert(False)\n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data variables for evaluation.\n",
    "'''\n",
    "\n",
    "folderName= net + 'net_imagenet_images_second'\n",
    "filterName= net +'net_imagenet_filters_second'\n",
    "dataFolder= net +'net_imagenet_data_second'\n",
    "\n",
    "failureFolderName='failure/' + folderName\n",
    "failureFilterName='failure/' + filterName\n",
    "\n",
    "dataName= 'second'\n",
    "\n",
    "#Job1\n",
    "dssim_data = []\n",
    "\n",
    "#Job5\n",
    "confRR_data = []\n",
    "confAR_data = []\n",
    "confRQ_data = []\n",
    "confAQ_data = []\n",
    "\n",
    "#Job4\n",
    "CW = [0]\n",
    "CC = [0]\n",
    "WC = [0]\n",
    "WW = [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,filename in enumerate(os.listdir(folderName)):\n",
    "\n",
    "    try:\n",
    "        filename.index(\"@\")\n",
    "    except:\n",
    "        print(filename)\n",
    "        continue\n",
    "        \n",
    "    if filename.endswith('.npy'):\n",
    "        count[0]+=1\n",
    "        CW[0] += 1\n",
    "        \n",
    "        numbers = filename[6:].split('@')\n",
    "        position = int(numbers[0])\n",
    "        total = int(numbers[1][:-4])\n",
    "        \n",
    "        print(position)\n",
    "        \n",
    "        image = np.load(folderName+\"/\"+filename)\n",
    "        fil = np.load(filterName+\"/\"+filename)\n",
    "        og = image - fil\n",
    "        \n",
    "        \n",
    "        job5(image,fil,og)\n",
    "        \n",
    "print(count[0])\n",
    "print(\"Processing Failures\")\n",
    "\n",
    "for i,filename in enumerate(os.listdir(failureFolderName)):\n",
    "    \n",
    "    try:\n",
    "        filename.index(\"@\")\n",
    "    except:\n",
    "        print(filename)\n",
    "        continue\n",
    "        \n",
    "    if filename.endswith('.npy'):\n",
    "        count[0]+=1\n",
    "        numbers = filename[6:].split('@')\n",
    "        position = int(numbers[0])\n",
    "        total = int(numbers[1][:-4])\n",
    "        \n",
    "        print(position)\n",
    "        \n",
    "        image = np.load(failureFolderName+\"/\"+filename)\n",
    "        fil = np.load(failureFilterName+\"/\"+filename)\n",
    "        og = image - fil\n",
    "        \n",
    "        job4(image,fil,og)\n",
    "        job5(image,fil,og)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysed and Saved Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert name of data file stored here to be graphed and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = open(\"./results/evaluation.csv\", \"w\")\n",
    "\n",
    "for element in confRR_data:\n",
    "    textfile.write(str(element) + \", \")\n",
    "\n",
    "textfile.write(\"\\n\")\n",
    "\n",
    "for element in confRQ_data:\n",
    "    textfile.write(str(element) + \", \")\n",
    "\n",
    "textfile.write(\"\\n\")\n",
    "\n",
    "for element in confAR_data:\n",
    "    textfile.write(str(element) + \", \")\n",
    "\n",
    "textfile.write(\"\\n\")\n",
    "\n",
    "for element in confAQ_data:\n",
    "    textfile.write(str(element) + \", \")\n",
    "\n",
    "textfile.write(\"\\n\")\n",
    "\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CC) # Both model predicted correctly\n",
    "print(CW) # Only full-precision model predicted correctly\n",
    "print(WC) # Only pruned/pruned+quantized model predicted correctly\n",
    "print(WW) # Both model predicted incorrectly\n",
    "print(CC[0]+WC[0]+CW[0]+WW[0])\n",
    "print(count[0]/3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dssim_data = np.array(dssim_data).astype(float)\n",
    "np.mean(dssim_data),np.std(dssim_data),np.max(dssim_data),len(dssim_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
