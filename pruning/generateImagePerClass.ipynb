{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import re\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "from tensorflow.python.framework.ops import enable_eager_execution\n",
    "#disable_eager_execution()\n",
    "enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "from tensorflow.keras.layers import Input\n",
    "import scipy.misc\n",
    "\n",
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultBNQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "        return []\n",
    "\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "        return []\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "        pass\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "        pass\n",
    "    def get_output_quantizers(self, layer):\n",
    "        return [tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=8, per_axis=False, symmetric=False, narrow_range=False)]\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n",
    "    \n",
    "    \n",
    "class NoOpQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    \"\"\"Use this config object if the layer has nothing to be quantized for \n",
    "    quantization aware training.\"\"\"\n",
    "\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "        return []\n",
    "\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "        return []\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "        pass\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "        pass\n",
    "\n",
    "    def get_output_quantizers(self, layer):\n",
    "        # Does not quantize output, since we return an empty list.\n",
    "        return []\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n",
    "    \n",
    "    \n",
    "def apply_quantization(layer):\n",
    "    if 'bn'  in layer.name:\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer,DefaultBNQuantizeConfig())\n",
    "    elif 'concat' in layer.name:\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer,NoOpQuantizeConfig())\n",
    "    else:\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(features):\n",
    "    \"\"\"Preprocesses the given image.\n",
    "       will convert the images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset, without scaling.\n",
    "       mean = [103.939, 116.779, 123.68]\n",
    "       std = None\n",
    "  \"\"\"\n",
    "    image = features[\"image\"]\n",
    "    image = tf.image.resize(image,[224,224])\n",
    "    features[\"image\"] = image\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "tfds_dataset2, tfds_info  = tfds.load(name='imagenet2012_subset', split='validation[-60%:]', with_info=True,\n",
    "                                     data_dir='../datasets/ImageNet/') # use the last 20% of images among 50000 validation images for testing\n",
    "#tf.compat.v1.data.make_one_shot_iterator(tfds_dataset1).get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tfds_dataset2.map(preprocess_image).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 224 ,224\n",
    "input_shape = (img_rows, img_cols, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model_ = ResNet50(input_shape=input_shape)\n",
    "res_model = ResNet50(input_tensor = res_model_.input)\n",
    "res_p_model = ResNet50(input_tensor=res_model_.input)\n",
    "res_pqat_model = tfmot.quantization.keras.quantize_model(res_model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res Done\n"
     ]
    }
   ],
   "source": [
    "res_model.load_weights(\"../weights/fp_model_40_resnet50.h5\")\n",
    "res_p_model.load_weights(\"../weights/p_model_40_resnet50.h5\")\n",
    "res_pqat_model.load_weights(\"../weights/pqat_model_40_resnet50.h5\")\n",
    "res_model.trainable = False\n",
    "res_p_model.trainable = False\n",
    "res_pqat_model.trainable = False\n",
    "res_model.compile()\n",
    "res_p_model.compile()\n",
    "res_pqat_model.compile()\n",
    "print(\"Res Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_model_ = MobileNet(input_shape=input_shape)\n",
    "mob_model = MobileNet(input_tensor = mob_model_.input)\n",
    "mob_p_model = MobileNet(input_tensor=mob_model_.input)\n",
    "mob_pqat_model = tfmot.quantization.keras.quantize_model(mob_model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mob Done\n"
     ]
    }
   ],
   "source": [
    "mob_model.load_weights(\"../weights/fp_model_40_mobilenet.h5\")\n",
    "mob_p_model.load_weights(\"../weights/p_model_40_mobilenet.h5\")\n",
    "mob_pqat_model.load_weights(\"../weights/pqat_model_40_mobilenet.h5\")\n",
    "mob_model.trainable = False\n",
    "mob_p_model.trainable = False\n",
    "mob_pqat_model.trainable = False\n",
    "mob_model.compile()\n",
    "mob_p_model.compile()\n",
    "mob_pqat_model.compile()\n",
    "print(\"Mob Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "den_model_ = DenseNet121(input_shape=input_shape)\n",
    "den_model = DenseNet121(input_tensor = den_model_.input)\n",
    "den_p_model = DenseNet121(input_tensor=den_model_.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Den Done\n"
     ]
    }
   ],
   "source": [
    "# Use `tf.keras.models.clone_model` to apply `apply_quantization_to_dense` \n",
    "# to the layers of the model.\n",
    "LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\n",
    "MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\n",
    "annotated_model = tf.keras.models.clone_model(\n",
    "    den_model_,\n",
    "    clone_function=apply_quantization,\n",
    ")\n",
    "\n",
    "with tfmot.quantization.keras.quantize_scope({'DefaultBNQuantizeConfig': DefaultBNQuantizeConfig, 'NoOpQuantizeConfig': NoOpQuantizeConfig}):\n",
    "    den_pqat_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "\n",
    "den_model.load_weights(\"../weights/fp_model_40_densenet121.h5\")\n",
    "den_p_model.load_weights(\"../weights/p_model_40_densenet121.h5\")\n",
    "den_pqat_model.load_weights(\"../weights/pqat_model_40_densenet121.h5\")\n",
    "den_model.trainable = False\n",
    "den_p_model.trainable = False\n",
    "den_pqat_model.trainable = False\n",
    "den_model.compile()\n",
    "den_p_model.compile()\n",
    "den_pqat_model.compile()\n",
    "print(\"Den Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(label_map, number):\n",
    "    progress = 0\n",
    "    for i in label_map.keys():\n",
    "        progress += len(label_map[i])\n",
    "    \n",
    "    print(progress/(number*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find images that all models agree on\n",
    "def work(image,file,label):\n",
    "    \n",
    "    # res\n",
    "    \n",
    "    res_orig_logist = res_model.predict(tf.keras.applications.resnet.preprocess_input(image.copy())[None, ...])\n",
    "    res_p_logist = res_p_model.predict(tf.keras.applications.resnet.preprocess_input(image.copy())[None, ...])\n",
    "    res_pqat_logist = res_pqat_model.predict(tf.keras.applications.resnet.preprocess_input(image.copy())[None, ...])\n",
    "    label1 = np.argmax(res_orig_logist)\n",
    "    label2 = np.argmax(res_p_logist)\n",
    "    label3 = np.argmax(res_pqat_logist)\n",
    "    \n",
    "    if not (label1 == label2 == label3 == label):\n",
    "        return False\n",
    "    \n",
    "    # mobile\n",
    "\n",
    "    image_copy = np.copy(image)\n",
    "\n",
    "    mob_orig_logist = mob_model.predict(tf.keras.applications.mobilenet.preprocess_input(image.copy())[None, ...])\n",
    "    mob_p_logist = mob_p_model.predict(tf.keras.applications.mobilenet.preprocess_input(image.copy())[None, ...])\n",
    "    mob_pqat_logist = mob_pqat_model.predict(tf.keras.applications.mobilenet.preprocess_input(image.copy())[None, ...])\n",
    "    label4 = np.argmax(mob_orig_logist)\n",
    "    label5 = np.argmax(mob_p_logist)\n",
    "    label6 = np.argmax(mob_pqat_logist)\n",
    "    \n",
    "    if not (label4 == label5 == label6 == label):\n",
    "        return False\n",
    "    \n",
    "    # dense\n",
    "    \n",
    "    image_copy = np.copy(image)\n",
    "    \n",
    "    den_orig_logist = den_model.predict(tf.keras.applications.densenet.preprocess_input(image.copy())[None, ...])\n",
    "    den_p_logist = den_p_model.predict(tf.keras.applications.densenet.preprocess_input(image.copy())[None, ...])\n",
    "    den_pqat_logists = den_pqat_model.predict(tf.keras.applications.densenet.preprocess_input(image.copy())[None, ...])\n",
    "    label7 = np.argmax(den_orig_logist)\n",
    "    label8 = np.argmax(den_p_logist)\n",
    "    label9 = np.argmax(den_pqat_logists)\n",
    "    \n",
    "    if not (label7 == label8 == label9 == label):\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_per_class = 3 # number of images per class\n",
    "\n",
    "for i,images in enumerate(val_ds):\n",
    "    \n",
    "    print(\"% OF IMAGES SEEN: \"+str(i/600))\n",
    "    \n",
    "    for j, image in enumerate(images['image']):\n",
    "\n",
    "        image = images['image'][j].numpy()\n",
    "        file = images['file_name'][j].numpy()\n",
    "        label = images['label'][j].numpy()\n",
    "        \n",
    "        if label not in label_map:\n",
    "            label_map[label] = []\n",
    "            \n",
    "        if work(image,file,label):\n",
    "            label_map[label] += [(image,file,label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 45, 1: 42, 2: 62, 3: 69, 4: 69, 5: 48, 6: 76, 7: 66, 8: 81, 9: 59, 10: 55, 11: 42, 12: 58, 13: 41, 14: 30, 15: 38, 16: 24, 17: 19, 18: 20, 19: 9, 20: 12, 21: 14, 22: 4, 23: 9, 24: 5, 25: 1, 27: 1, 30: 1}\n"
     ]
    }
   ],
   "source": [
    "# k: v -> v classes has k images that all models agree on\n",
    "d = {}\n",
    "for c in label_map:\n",
    "    d[len(label_map[c])] = d.get(len(label_map[c]), 0) + 1\n",
    "print(dict(sorted(d.items(), key=lambda x:x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label_map = {}\n",
    "extra = []\n",
    "for c in label_map:\n",
    "    if len(label_map[c]) >= num_per_class:\n",
    "        new_label_map[c] = label_map[c][:num_per_class]\n",
    "        if len(label_map[c]) > num_per_class:\n",
    "            extra.append((c, label_map[c][num_per_class]))\n",
    "    else:\n",
    "        new_label_map[c] = label_map[c]\n",
    "        for i in range(num_per_class-len(label_map[c])):\n",
    "            print(i, len(extra))\n",
    "            new_label_map[extra[0][0]].append(extra[0][1])\n",
    "            extra = extra[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 45, 1: 42, 2: 62, 3: 570, 4: 281}\n"
     ]
    }
   ],
   "source": [
    "# k: v -> v classes has k images that all models agree on\n",
    "d = {}\n",
    "for c in new_label_map:\n",
    "    d[len(new_label_map[c])] = d.get(len(new_label_map[c]), 0) + 1\n",
    "print(dict(sorted(d.items(), key=lambda x:x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for c in new_label_map:\n",
    "    cnt += len(new_label_map[c])\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = []\n",
    "image_data = []\n",
    "label_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in new_label_map:\n",
    "    try:\n",
    "        a = [s[0] for s in new_label_map[i]]\n",
    "        b = [s[1] for s in new_label_map[i]]\n",
    "        c = [s[2] for s in new_label_map[i]]\n",
    "        image_data = image_data + a\n",
    "        file_data = file_data + b\n",
    "        label_data = label_data + c\n",
    "    except:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "print(len(image_data))\n",
    "\n",
    "file_data_ = np.array(file_data)\n",
    "image_data_ = np.array(image_data)\n",
    "label_data_ = np.array(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "KImagePerClass = tf.data.Dataset.from_tensor_slices({\"file_name\":file_data_,\"image\":image_data_, \"label\":label_data_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(\n",
    "    KImagePerClass, \"../datasets/ImageNet/pruning/3kImages\", compression=None, shard_func=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_name': TensorSpec(shape=(), dtype=tf.string, name=None), 'image': TensorSpec(shape=(224, 224, 3), dtype=tf.float32, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}\n"
     ]
    }
   ],
   "source": [
    "print(KImagePerClass.element_spec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
