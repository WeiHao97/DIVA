{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import png\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert path files appropriately for data which can be found after generation in results folder for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = \"res\"\n",
    "attack = \"PGD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"1\" # Hyperparameter C value from data\n",
    "locald = '../ImageNet/results/'+ attack +'/DIVA/'+net+'net/' # Please change to the corresponding results folder for evaluation\n",
    "folderName= net + 'net_imagenet_images_second'\n",
    "filterName= net +'net_imagenet_filters_second'\n",
    "dataFolder= net +'net_imagenet_data_second'\n",
    "dataName= 'second'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats():\n",
    "    advist_data = np.array([])\n",
    "\n",
    "    with open(locald + dataFolder+\"/\"+dataName+'_advdist_data.csv', 'r') as filehandle:\n",
    "        for line in filehandle:\n",
    "            temp_data = line[:-2]\n",
    "\n",
    "        advist_data = np.concatenate((advist_data, np.array(temp_data.split(\", \"))))\n",
    "        \n",
    "    steps_data = np.array([])\n",
    "\n",
    "    with open(locald +dataFolder+\"/\"+dataName+'_steps_data.csv', 'r') as filehandle:\n",
    "        for line in filehandle:\n",
    "            temp_data = line[:-2]\n",
    "\n",
    "        steps_data = np.concatenate((steps_data, np.array(temp_data.split(\", \"))))\n",
    "    \n",
    "    time_data = np.array([])\n",
    "\n",
    "    with open(locald +dataFolder+\"/\"+dataName+'_time_data.csv', 'r') as filehandle:\n",
    "        for line in filehandle:\n",
    "            temp_data = line[:-2]\n",
    "\n",
    "        time_data = np.concatenate((time_data, np.array(temp_data.split(\", \"))))\n",
    "    \n",
    "    advist_datak = np.array([])\n",
    "\n",
    "    with open(locald +dataFolder+\"/\"+dataName+'_advdistk_data.csv', 'r') as filehandle:\n",
    "        for line in filehandle:\n",
    "            temp_data = line[:-2]\n",
    "\n",
    "        advist_datak = np.concatenate((advist_datak, np.array(temp_data.split(\", \"))))\n",
    "        \n",
    "    steps_datak = np.array([])\n",
    "\n",
    "    with open(locald +dataFolder+\"/\"+dataName+'_stepsk_data.csv', 'r') as filehandle:\n",
    "        for line in filehandle:\n",
    "            temp_data = line[:-2]\n",
    "\n",
    "        steps_datak = np.concatenate((steps_datak, np.array(temp_data.split(\", \"))))\n",
    "    \n",
    "    time_datak = np.array([])\n",
    "\n",
    "    with open(locald +dataFolder+\"/\"+dataName+'_timek_data.csv', 'r') as filehandle:\n",
    "        for line in filehandle:\n",
    "            temp_data = line[:-2]\n",
    "\n",
    "        time_datak = np.concatenate((time_datak, np.array(temp_data.split(\", \"))))\n",
    "    \n",
    "    time_data = time_data.astype('float')\n",
    "    advist_data = advist_data.astype('float')\n",
    "    steps_data = steps_data.astype('float')\n",
    "\n",
    "    time_datak = time_datak.astype('float')\n",
    "    advist_datak = advist_datak.astype('float')\n",
    "    steps_datak = steps_datak.astype('float')\n",
    "\n",
    "    time_data_ = np.mean(time_data), np.std(time_data)\n",
    "    advdist_data_ = np.mean(advist_data), np.std(advist_data)\n",
    "    steps_data_ = np.mean(steps_data), np.std(steps_data)\n",
    "\n",
    "    time_datak_ = np.mean(time_datak), np.std(time_datak)\n",
    "    advdist_datak_ = np.mean(advist_datak), np.std(advist_datak)\n",
    "    steps_datak_ = np.mean(steps_datak), np.std(steps_datak)\n",
    "    \n",
    "    print(\"Number of Successes\",len(steps_data))\n",
    "    print(\"Total Time\",np.sum(time_data)) \n",
    "    print(\"Total Steps\",np.sum(steps_data))\n",
    "    print(\"Steps\",steps_data_)\n",
    "    print(\"Time\", time_data_)\n",
    "    \n",
    "    print()\n",
    "    print(\"Time5\",time_datak_) \n",
    "    print(\"Dist5\",advdist_datak_) \n",
    "    print(\"Steps5\",steps_datak_)\n",
    "    \n",
    "    print()\n",
    "    print(\"Number of top1 success\",(len(steps_data))/3000)\n",
    "    print(\"Number of top5 success\",len(steps_datak)/3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"c\", c)\n",
    "print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Custom layers for DenseNet to support quantisation\n",
    "'''\n",
    "\n",
    "class DefaultBNQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "        return []\n",
    "\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "        return []\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "        pass\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "        pass\n",
    "    def get_output_quantizers(self, layer):\n",
    "        return [tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=8, per_axis=False, symmetric=False, narrow_range=False)]\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n",
    "    \n",
    "    \n",
    "class NoOpQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    \"\"\"Use this config object if the layer has nothing to be quantized for \n",
    "    quantization aware training.\"\"\"\n",
    "\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "        return []\n",
    "\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "        return []\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "        pass\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "        pass\n",
    "\n",
    "    def get_output_quantizers(self, layer):\n",
    "        # Does not quantize output, since we return an empty list.\n",
    "        return []\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n",
    "    \n",
    "    \n",
    "def apply_quantization(layer):\n",
    "    if 'bn'  in layer.name:\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer,DefaultBNQuantizeConfig())\n",
    "    elif 'concat' in layer.name:\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer,NoOpQuantizeConfig())\n",
    "    else:\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 224 ,224\n",
    "input_shape = (img_rows, img_cols, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if net == \"res\":\n",
    "    model_ = ResNet50(input_shape=input_shape)\n",
    "    q_model = tfmot.quantization.keras.quantize_model(model_)\n",
    "    model = ResNet50(input_tensor = q_model.input)\n",
    "    model.load_weights(\"../../weights/fp_model_40_resnet50.h5\")\n",
    "    q_model.load_weights(\"../../weights/q_model_40_resnet50.h5\")\n",
    "    print(\"ResNet Done\")\n",
    "    \n",
    "elif net == \"mobile\":\n",
    "    model_ = MobileNet(input_shape=input_shape)\n",
    "    q_model = tfmot.quantization.keras.quantize_model(model_)\n",
    "    model = MobileNet(input_tensor = q_model.input)\n",
    "    model.load_weights(\"../../weights/fp_model_40_mobilenet.h5\")\n",
    "    q_model.load_weights(\"../../weights/q_model_40_mobilenet.h5\")\n",
    "    print(\"MobNet Done\")\n",
    "    \n",
    "else:\n",
    "    model_ = tf.keras.applications.DenseNet121(input_shape=(img_rows, img_cols,3))\n",
    "    # Create a base model\n",
    "    base_model = model_\n",
    "    # Helper function uses `quantize_annotate_layer` to annotate that only the \n",
    "    # Dense layers should be quantized.\n",
    "\n",
    "    LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\n",
    "    MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\n",
    "\n",
    "    # Use `tf.keras.models.clone_model` to apply `apply_quantization_to_dense` \n",
    "    # to the layers of the model.\n",
    "    annotated_model = tf.keras.models.clone_model(\n",
    "        base_model,\n",
    "        clone_function=apply_quantization,\n",
    "    )\n",
    "\n",
    "    with tfmot.quantization.keras.quantize_scope({'DefaultBNQuantizeConfig': DefaultBNQuantizeConfig, 'NoOpQuantizeConfig': NoOpQuantizeConfig}):\n",
    "        q_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "\n",
    "    model = DenseNet121(input_tensor = q_model.input)\n",
    "    model.load_weights(\"../../weights/fp_model_40_densenet121.h5\")\n",
    "    q_model.load_weights(\"../../weights/q_model_40_densenet121.h5\")\n",
    "    print(\"DenseNet Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = False\n",
    "q_model.trainable = False\n",
    "model.compile() \n",
    "q_model.compile() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appropriate Preprocessing and Decoding predictions\n",
    "if net==\"res\":\n",
    "    preproc = tf.keras.applications.resnet.preprocess_input\n",
    "elif net==\"mobile\":\n",
    "    preproc = tf.keras.applications.mobilenet.preprocess_input\n",
    "else:\n",
    "    preproc = tf.keras.applications.densenet.preprocess_input\n",
    "\n",
    "    #Choose model\n",
    "if net==\"res\":\n",
    "    dec = tf.keras.applications.resnet.decode_predictions\n",
    "elif net==\"mobile\":\n",
    "    dec = tf.keras.applications.mobilenet.decode_predictions\n",
    "else:\n",
    "    dec = tf.keras.applications.densenet.decode_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Generated Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = {'file_name': tf.TensorSpec(shape=(), dtype=tf.string, name=None),\n",
    " 'image': tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32, name=None),\n",
    " 'label': tf.TensorSpec(shape=(), dtype=tf.int64, name=None)}\n",
    "\n",
    "mydataset = tf.data.experimental.load(\"../../datasets/Imagenet/quantisation/3kImages/\",es).batch(20).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DSSIM Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this job you must have dssim downloaded which can be done byusing their github link, https://github.com/kornelski/dssim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job1(image,fil,og):\n",
    "    \n",
    "    tf.keras.preprocessing.image.save_img(\"./fake1.png\", image)\n",
    "    tf.keras.preprocessing.image.save_img(\"./real1.png\", og)\n",
    "    \n",
    "    process = subprocess.run(['./path/to/dssim','./fake1.png','./real1.png'], \n",
    "                         stdout=subprocess.PIPE,\n",
    "                         stderr=subprocess.PIPE,\n",
    "                         universal_newlines=True)\n",
    "    if process.stdout.split('\\t')[0] == '':\n",
    "        print(process.stderr)\n",
    "    else:\n",
    "        dssim_data.append(process.stdout.split('\\t')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Delta Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job2(image, fil, og):\n",
    "    \n",
    "    image = np.copy(image)\n",
    "    og = np.copy(og)\n",
    "    \n",
    "    real = np.expand_dims(og, axis=0)\n",
    "    attack = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    pred1, pred2= model.predict(real), q_model.predict(real)\n",
    "    \n",
    "    real_r_pred = dec(pred1, top=1000)\n",
    "    real_q_pred = dec(pred2, top=1000)\n",
    "    \n",
    "    pred1, pred2= model.predict(attack), q_model.predict(attack)\n",
    "    \n",
    "    attack_r_pred = dec(pred1, top=1000)\n",
    "    attack_q_pred = dec(pred2, top=1000)\n",
    "    \n",
    "    r_label = real_r_pred[0][0][1]\n",
    "    \n",
    "    print(r_label)\n",
    "    \n",
    "    rr = float(real_r_pred[0][0][2]) #Correct Label \n",
    "    rq = float(real_q_pred[0][0][2]) #Correct Label \n",
    "    \n",
    "    for i in range(0,1000):\n",
    "        if attack_q_pred[0][i][1] == r_label:\n",
    "            aq = float(attack_q_pred[0][i][2])\n",
    "            break\n",
    "                       \n",
    "    for i in range(0,1000):\n",
    "        if attack_r_pred[0][i][1] == r_label:\n",
    "            ar = float(attack_r_pred[0][i][2])\n",
    "            break\n",
    "    \n",
    "    \n",
    "    confRR_data.append(rr)\n",
    "    confAR_data.append(ar)\n",
    "    confRQ_data.append(rq)\n",
    "    confAQ_data.append(aq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job3(image,fil,og):\n",
    "    \n",
    "    image = np.copy(image)\n",
    "    og = np.copy(og)\n",
    "      \n",
    "    real = np.expand_dims(preproc(og), axis=0)\n",
    "    attack = np.expand_dims(preproc(image), axis=0)\n",
    "    \n",
    "    pred1, pred2= model.predict(real), q_model.predict(real)\n",
    "    \n",
    "    r_label = np.argmax(pred1)\n",
    "    \n",
    "    pred1, pred2= model.predict(attack), q_model.predict(attack)\n",
    "    \n",
    "    attack_r_pred = np.argmax(pred1)\n",
    "    attack_q_pred = np.argmax(pred2)\n",
    "    \n",
    "    if (r_label == attack_r_pred and r_label == attack_q_pred):\n",
    "        CC[0] += 1\n",
    "        return\n",
    "    if (r_label != attack_r_pred and r_label != attack_q_pred):\n",
    "        WW[0] += 1\n",
    "        return\n",
    "    if (r_label != attack_r_pred and r_label == attack_q_pred):\n",
    "        WC[0] += 1\n",
    "        return\n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data variables for evaluation. Please fill in the appropriate data folder names for the different models in order to evaluate\n",
    "'''\n",
    "\n",
    "folderName= net + 'net_imagenet_images_second'\n",
    "filterName= net +'net_imagenet_filters_second'\n",
    "dataFolder= net +'net_imagenet_data_second'\n",
    "\n",
    "failureFolderName='failure/' + folderName\n",
    "failureFilterName='failure/' + filterName\n",
    "\n",
    "dataName= 'second'\n",
    "\n",
    "#Job1\n",
    "dssim_data = []\n",
    "\n",
    "#Job5\n",
    "confRR_data = []\n",
    "confAR_data = []\n",
    "confRQ_data = []\n",
    "confAQ_data = []\n",
    "\n",
    "#Job4\n",
    "CW = [0]\n",
    "CC = [0]\n",
    "WC = [0]\n",
    "WW = [0]\n",
    "\n",
    "count = [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Files for images and filters are found and the original image is rebuilt, which is then used for further analysis\n",
    "'''\n",
    "\n",
    "for i,filename in enumerate(os.listdir(locald + folderName)):\n",
    "    continue\n",
    "\n",
    "    try:\n",
    "        filename.index(\"@\")\n",
    "    except:\n",
    "        print(filename)\n",
    "        continue\n",
    "        \n",
    "    if filename.endswith('.npy'):\n",
    "        count[0]+=1\n",
    "        CW[0] += 1\n",
    "        \n",
    "        numbers = filename[6:].split('@')\n",
    "        position = int(numbers[0])\n",
    "        total = int(numbers[1][:-4])\n",
    "        \n",
    "        print(position)\n",
    "        \n",
    "        image = np.load(locald +folderName+\"/\"+filename)\n",
    "        fil = np.load(locald +filterName+\"/\"+filename)\n",
    "        og = image - fil\n",
    "        \n",
    "        job1(image,fil, og)\n",
    "        job2(image,fil,og)\n",
    "        \n",
    "print(count[0])\n",
    "print(\"Processing Failures\")\n",
    "\n",
    "for i,filename in enumerate(os.listdir(locald + failureFolderName)):\n",
    "    \n",
    "    try:\n",
    "        filename.index(\"@\")\n",
    "    except:\n",
    "        print(filename)\n",
    "        continue\n",
    "        \n",
    "    if filename.endswith('.npy'):\n",
    "        count[0]+=1\n",
    "        numbers = filename[6:].split('@')\n",
    "        position = int(numbers[0])\n",
    "        total = int(numbers[1][:-4])\n",
    "        \n",
    "        print(position)\n",
    "        \n",
    "        image = np.load(locald +failureFolderName+\"/\"+filename)\n",
    "        fil = np.load(locald +failureFilterName+\"/\"+filename)\n",
    "        og = image - fil\n",
    "        \n",
    "        job2(image,fil,og)\n",
    "        job3(image,fil,og)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysed and Saved Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert name of data file stored here to be graphed and visualised later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = open(\"../Imagenet/Results/INSERT_NAME.csv\", \"w\")\n",
    "\n",
    "for element in confRR_data:\n",
    "    textfile.write(str(element) + \", \")\n",
    "\n",
    "textfile.write(\"\\n\")\n",
    "\n",
    "for element in confRQ_data:\n",
    "    textfile.write(str(element) + \", \")\n",
    "\n",
    "textfile.write(\"\\n\")\n",
    "\n",
    "for element in confAR_data:\n",
    "    textfile.write(str(element) + \", \")\n",
    "\n",
    "textfile.write(\"\\n\")\n",
    "\n",
    "for element in confAQ_data:\n",
    "    textfile.write(str(element) + \", \")\n",
    "\n",
    "textfile.write(\"\\n\")\n",
    "\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CC)\n",
    "print(CW)\n",
    "print(WC)\n",
    "print(WW)\n",
    "print(CC[0]+WC[0]+CW[0]+WW[0])\n",
    "print(count[0]/3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dssim_data = np.array(dssim_data).astype(float)\n",
    "np.mean(dssim_data),np.std(dssim_data),np.max(dssim_data),len(dssim_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
