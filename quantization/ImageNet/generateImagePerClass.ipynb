{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "from tensorflow.keras.layers import Input\n",
    "import scipy.misc\n",
    "\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import re\n",
    "from tensorflow.python.framework.ops import enable_eager_execution\n",
    "enable_eager_execution()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"5\"\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(features):\n",
    "    \"\"\"Preprocesses the given image.\n",
    "       will convert the images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset, without scaling.\n",
    "       mean = [103.939, 116.779, 123.68]\n",
    "       std = None\n",
    "  \"\"\"\n",
    "    image = features[\"image\"]\n",
    "    image = tf.image.resize(image,[224,224])\n",
    "    features[\"image\"] = image\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds_dataset2, tfds_info  = tfds.load(name='imagenet2012_subset', split='validation[-60%:]', with_info=True,\n",
    "                                     data_dir='../../datasets/ImageNet') # use the last 20% of images among 50000 validation images for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = tfds.show_examples(tfds_dataset2, tfds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tfds_dataset2.map(preprocess_image).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 224 ,224\n",
    "input_shape = (img_rows, img_cols, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet Model\n",
    "\n",
    "model_ = ResNet50(input_shape=input_shape)\n",
    "q_model = tfmot.quantization.keras.quantize_model(model_)\n",
    "model = ResNet50(input_tensor = q_model.input)\n",
    "model.load_weights(\"../../weights/fp_model_resnet50.h5\")\n",
    "q_model.load_weights(\"../../weights/q_model_resnet50.h5\")\n",
    "model.trainable = False\n",
    "q_model.trainable = False\n",
    "sb_model = ResNet50(input_tensor = q_model.input)\n",
    "sb_model.load_weights(\"../../weights/d_model_resnet50.h5\")\n",
    "sb_model.trainable = False\n",
    "print(\"ResNet Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()\n",
    "q_model.compile()\n",
    "sb_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mobilenet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MobileNet Model\n",
    "\n",
    "mob_model_ = MobileNet(input_shape=input_shape)\n",
    "mob_q_model = tfmot.quantization.keras.quantize_model(mob_model_)\n",
    "mob_model = MobileNet(input_tensor = mob_q_model.input)\n",
    "mob_model.load_weights(\"../../weights/fp_model_mobilenet.h5\")\n",
    "mob_q_model.load_weights(\"../../weights/q_model_mobilenet.h5\")\n",
    "mob_model.trainable = False\n",
    "mob_q_model.trainable = False\n",
    "sb_mob_model = MobileNet(input_tensor = mob_q_model.input)\n",
    "sb_mob_model.load_weights(\"../../weights/d_model_mobilenet.h5\")\n",
    "sb_mob_model.trainable = False\n",
    "print(\"MobileNet Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mob_model.compile()\n",
    "mob_q_model.compile()\n",
    "sb_mob_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Custom DenseNet layers to support quantization\n",
    "\n",
    "class DefaultBNQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "        return []\n",
    "\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "        return []\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "        pass\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "        pass\n",
    "    def get_output_quantizers(self, layer):\n",
    "        return [tfmot.quantization.keras.quantizers.MovingAverageQuantizer(num_bits=8, per_axis=False, symmetric=False, narrow_range=False)]\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n",
    "    \n",
    "    \n",
    "class NoOpQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    \"\"\"Use this config object if the layer has nothing to be quantized for \n",
    "    quantization aware training.\"\"\"\n",
    "\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "        return []\n",
    "\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "        return []\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "        pass\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "        pass\n",
    "\n",
    "    def get_output_quantizers(self, layer):\n",
    "        # Does not quantize output, since we return an empty list.\n",
    "        return []\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n",
    "    \n",
    "    \n",
    "def apply_quantization(layer):\n",
    "    if 'bn'  in layer.name:\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer,DefaultBNQuantizeConfig())\n",
    "    elif 'concat' in layer.name:\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer,NoOpQuantizeConfig())\n",
    "    else:\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model_ = tf.keras.applications.DenseNet121(input_shape=(img_rows, img_cols,3))\n",
    "# Create a base model\n",
    "base_model = dense_model_\n",
    "# Helper function uses `quantize_annotate_layer` to annotate that only the \n",
    "# Dense layers should be quantized.\n",
    "\n",
    "LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\n",
    "MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\n",
    "\n",
    "# Use `tf.keras.models.clone_model` to apply `apply_quantization_to_dense` \n",
    "# to the layers of the model.\n",
    "annotated_model = tf.keras.models.clone_model(\n",
    "    base_model,\n",
    "    clone_function=apply_quantization,\n",
    ")\n",
    "\n",
    "with tfmot.quantization.keras.quantize_scope({'DefaultBNQuantizeConfig': DefaultBNQuantizeConfig, 'NoOpQuantizeConfig': NoOpQuantizeConfig}):\n",
    "    dense_q_model = tfmot.quantization.keras.quantize_apply(annotated_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model = tf.keras.applications.DenseNet121(input_tensor = q_model.input)\n",
    "dense_model.load_weights(\"../../weights/fp_model_densenet.h5\")\n",
    "dense_q_model.load_weights(\"../../weights/q_model_densenet.h5\")\n",
    "dense_model.trainable = False\n",
    "dense_q_model.trainable = False\n",
    "sb_dense_model = DenseNet121(input_tensor = dense_q_model.input)\n",
    "sb_dense_model.load_weights(\"../../weights/d_model_densenet.h5\")\n",
    "sb_dense_model.trainable = False\n",
    "print(\"Dense Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model.compile()\n",
    "dense_q_model.compile()\n",
    "sb_dense_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {} #Mapping from label images to count\n",
    "number = 4 #Number of images per class group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(label_map, number):\n",
    "    progress = 0\n",
    "    for i in label_map.keys():\n",
    "        progress += len(label_map[i])\n",
    "    \n",
    "    print(progress/(number*1000))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task of creating images that all agree on the same label\n",
    "def work(image,file,label):\n",
    "    \n",
    "    image_copy = np.copy(image)\n",
    "    \n",
    "    res_image_ =  np.expand_dims(tf.keras.applications.resnet.preprocess_input(image_copy), axis=0)\n",
    "        \n",
    "    orig_logist = model.predict(res_image_)\n",
    "    q_logist = q_model.predict(res_image_)\n",
    "    orig_logists = sb_model.predict(res_image_)\n",
    "    label1 = np.argmax(orig_logist)\n",
    "    label2 = np.argmax(q_logist)\n",
    "    label3 = np.argmax(orig_logists)\n",
    "    \n",
    "    image_copy = np.copy(image)\n",
    "    \n",
    "    mob_image_ =  np.expand_dims(tf.keras.applications.mobilenet.preprocess_input(image_copy), axis=0)\n",
    "        \n",
    "    orig_logist = mob_model.predict(mob_image_)\n",
    "    q_logist = mob_q_model.predict(mob_image_)\n",
    "    orig_logists = sb_mob_model.predict(mob_image_)\n",
    "    label4 = np.argmax(orig_logist)\n",
    "    label5 = np.argmax(q_logist)\n",
    "    label6 = np.argmax(orig_logists)\n",
    "    \n",
    "    image_copy = np.copy(image)\n",
    "    \n",
    "    den_image_ =  np.expand_dims(tf.keras.applications.densenet.preprocess_input(image_copy), axis=0)\n",
    "        \n",
    "    orig_logist = dense_model.predict(den_image_)\n",
    "    q_logist = dense_q_model.predict(den_image_)\n",
    "    orig_logists = sb_dense_model.predict(den_image_)\n",
    "    label7 = np.argmax(orig_logist)\n",
    "    label8 = np.argmax(q_logist)\n",
    "    label9 = np.argmax(orig_logists)\n",
    "    \n",
    "    # We generate all the labels and compare them in order to create a suitable dataset\n",
    "    all_labels = set([label1,label2,label3,label4,label5,label6,label7,label8,label9,label])\n",
    "    \n",
    "    if len(all_labels) != 1:\n",
    "        print(\"Res\",[label1,label2,label3])\n",
    "        print(\"Mob\",[label4,label5,label6])\n",
    "        print(\"Dense\",[label7,label8,label9])\n",
    "        print(\"Correct\", label)\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_data():\n",
    "    for i,images in enumerate(val_ds):\n",
    "    \n",
    "    print(\"% OF IMAGES SEEN: \"+str(i/600))\n",
    "    \n",
    "    for j, image in enumerate(images['image']):\n",
    "\n",
    "        image = images['image'][j].numpy()\n",
    "        file = images['file_name'][j].numpy()\n",
    "        label = images['label'][j].numpy()\n",
    "        \n",
    "        if label not in label_map.keys():\n",
    "            \n",
    "            if work(image,file,label):\n",
    "                continue\n",
    "            print(\"found:\" + str(label))\n",
    "            label_map[label] = [(image,file,label)]\n",
    "            \n",
    "        elif len(label_map[label]) < number:\n",
    "            \n",
    "            if work(image,file,label):\n",
    "                continue\n",
    "                \n",
    "            label_map[label] = label_map[label] + [(image,file,label)]\n",
    "            print(\"count:\" + str(label)+\",\"+str(len(label_map[label])))\n",
    "    \n",
    "    check(label_map,number)\n",
    "    \n",
    "    file_data = []\n",
    "    image_data = []\n",
    "    label_data = []\n",
    "    \n",
    "    for i in range(0,1000):\n",
    "        try:\n",
    "            a = [s[0] for s in label_map[i]]\n",
    "            b = [s[1] for s in label_map[i]]\n",
    "            c = [s[2] for s in label_map[i]]\n",
    "            image_data = image_data + a[:3]\n",
    "            file_data = file_data + b[:3]\n",
    "            label_data = label_data + c[:3]\n",
    "        except:\n",
    "            print(i)\n",
    "        \n",
    "    for i in range(0,1000):\n",
    "        try:\n",
    "            a = [s[0] for s in label_map[i]]\n",
    "            b = [s[1] for s in label_map[i]]\n",
    "            c = [s[2] for s in label_map[i]]\n",
    "            image_data = image_data + [a[3]]\n",
    "            file_data = file_data + [b[3]]\n",
    "            label_data = label_data + [c[3]]\n",
    "        except:\n",
    "            print(i)\n",
    "\n",
    "        if len(image_data) == 3000:\n",
    "            print(\"DONE\")\n",
    "            break\n",
    "        \n",
    "        print(len(image_data))\n",
    "\n",
    "    file_data_ = np.array(file_data)\n",
    "    image_data_ = np.array(image_data)\n",
    "    label_data_ = np.array(label_data)\n",
    "    \n",
    "    KImagePerClass = tf.data.Dataset.from_tensor_slices({\"file_name\":file_data_,\"image\":image_data_, \"label\":label_data_})\n",
    "    tf.data.experimental.save(KImagePerClass, \"../../datasets/Imagenet/quantisation/3KImagePerClass\", compression=None, shard_func=None)\n",
    "    print(KImagePerClass.element_spec)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_new_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test loading new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = {'file_name': tf.TensorSpec(shape=(), dtype=tf.string, name=None),\n",
    " 'image': tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32, name=None),\n",
    " 'label': tf.TensorSpec(shape=(), dtype=tf.int64, name=None)}\n",
    "mydataset = tf.data.experimental.load(\"../../datasets/Imagenet/quantisation/3KImagePerClass\",es).batch(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
